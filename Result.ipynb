{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import minerl\n",
    "from stable_baselines.common.policies import MlpPolicy, CnnPolicy, CnnLnLstmPolicy\n",
    "from stable_baselines.common import make_vec_env\n",
    "from stable_baselines import A2C\n",
    "from Wrappers import ActionWrapper, ObsWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "env = gym.make('MineRLNavigateDense-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2C-C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "model_c = A2C.load(\"a2c_mineRL_MLP_2\")\n",
    "env_c1 = gym.wrappers.FlattenObservation(gym.wrappers.FilterObservation(env, [\"compassAngle\"]))\n",
    "env_c2 = ActionWrapper(env_c1)\n",
    "\n",
    "nr_steps = 1000\n",
    "nr_iter = 10\n",
    "reward_list_m_c = np.zeros((nr_iter, nr_steps))\n",
    "\n",
    "for j in range(nr_iter):\n",
    "    obs = env_c2.reset()\n",
    "    done = False\n",
    "    for i in range(nr_steps):\n",
    "        action, _states = model_c.predict(obs)\n",
    "        obs, reward, done, info = env_c2.step(action)\n",
    "        reward_list_m_c[j,i] = reward\n",
    "        env.render()\n",
    "        if done:\n",
    "            print(\"Done\", done)\n",
    "            break\n",
    "    print(\"Iter {0} done!\".format(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for i in range(nr_iter):\n",
    "    plt.plot(np.cumsum(reward_list_m_c[i,:]))\n",
    "\n",
    "plt.title(\"A2C-C\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2C-CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "env_ci = ObsWrapper((64, 64), env)\n",
    "env_ci = ActionWrapper(env_ci)\n",
    "model_ci = A2C.load(\"a2c_mineRL_CnnMLP_2\")\n",
    "\n",
    "nr_steps = 1000\n",
    "nr_iter = 10\n",
    "reward_list_m_ci = np.zeros((nr_iter, nr_steps))\n",
    "tot_rew = 0\n",
    "\n",
    "for j in range(nr_iter):\n",
    "    obs = env_ci.reset()\n",
    "    done = False\n",
    "    for i in range(nr_steps):\n",
    "        action, _states = model_ci.predict(obs)\n",
    "        obs, reward, done, info = env_ci.step(action)\n",
    "        reward_list_m_ci[j,i] = reward\n",
    "        tot_rew += reward\n",
    "        env.render()\n",
    "        if done:\n",
    "            break\n",
    "    print(\"Iter {0}, {1} done!\".format(j, tot_rew))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for i in range(nr_iter):\n",
    "    plt.plot(np.cumsum(reward_list_m_ci[i,:]))\n",
    "\n",
    "plt.title(\"A2C-CI\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "nr_steps = 1000\n",
    "nr_iter = 10\n",
    "reward_list_rand = np.zeros((nr_iter, nr_steps))\n",
    "\n",
    "for j in range(nr_iter):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    net_reward = 0\n",
    "    for i in range(nr_steps):\n",
    "        action = env.action_space.sample()\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        reward_list_rand[j,i] = reward\n",
    "        if done:\n",
    "            break\n",
    "    print(\"Iter {0} done!\".format(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for i in range(nr_iter):\n",
    "    plt.plot(np.cumsum(reward_list_rand[i,:]))\n",
    "\n",
    "plt.title(\"Random\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "cum_vec_m_c = np.cumsum(reward_list_m_c,axis = 1)\n",
    "mean_vec_m_c = np.mean(cum_vec_m_c,axis = 0)\n",
    "std_vec_m_c = np.std(cum_vec_m_c,axis = 0)\n",
    "\n",
    "cum_vec_m_ci = np.cumsum(reward_list_m_ci,axis = 1)\n",
    "mean_vec_m_ci = np.mean(cum_vec_m_ci,axis = 0)\n",
    "std_vec_m_ci = np.std(cum_vec_m_ci,axis = 0)\n",
    "\n",
    "cum_vec_rand = np.cumsum(reward_list_rand,axis = 1)\n",
    "mean_vec_rand = np.mean(cum_vec_rand,axis = 0)\n",
    "std_vec_rand = np.std(cum_vec_rand,axis = 0)\n",
    "\n",
    "plt.plot(mean_vec_m_c,label = 'Cumulative reward AC2-C agent')\n",
    "plt.fill_between(np.arange(1000),mean_vec_m_c-std_vec_m_c,mean_vec_m_c+std_vec_m_c,alpha = 0.5)\n",
    "\n",
    "plt.plot(mean_vec_m_ci,label = 'Cumulative reward AC2-CI agent')\n",
    "plt.fill_between(np.arange(1000),mean_vec_m_ci-std_vec_m_ci,mean_vec_m_ci+std_vec_m_ci,alpha = 0.5)\n",
    "\n",
    "plt.plot(mean_vec_rand,label = 'Cumulative reward random agent')\n",
    "plt.fill_between(np.arange(1000),mean_vec_rand-std_vec_rand,mean_vec_rand+std_vec_rand,alpha = 0.5)\n",
    "plt.legend()\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('Reward')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MineRL] *",
   "language": "python",
   "name": "conda-env-MineRL-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
